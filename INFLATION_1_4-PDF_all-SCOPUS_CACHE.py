#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
INFLATION_META_ANALYSIS_v6_hybrid_caching.py
Hybridn√≠ architektura vyu≈æ√≠vaj√≠c√≠ Prompt Caching + Extended Thinking
"""

import logging
import sys
import os
import tkinter as tk
from tkinter import filedialog
import pandas as pd
import PyPDF2
import anthropic
import datetime
import time
from typing import List, Dict, Optional, Tuple, Any
from pathlib import Path
import hashlib
import json

# Nastaven√≠ loggingu
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

# Naƒçten√≠ konfigurace
try:
    from config import CLAUDE_API_KEY, SCOPUS_API_KEY, CLAUDE_MODEL
except ImportError:
    logger.error("‚ùå Chyba: Soubor config.py nebyl nalezen!")
    sys.exit(1)

# Definice sloupc≈Ø
META_ANALYSIS_COLUMNS = [
    "Idstudy", "IdEstimate", "Author", "Author_Affiliation", "DOI", "Journal_Name", 
    "Num_Citations", "Year", "Base_Model_Type", "Augmented_base_model", 
    "Augmentation_Description", "Ramsey_Rule", "HH_Included", "Firms_Included", 
    "Banks_Included", "Government_Included", "HH_Maximization_Type", 
    "HH_Maximized_Vars", "Producer_Type", "Producer_Assumption", 
    "Other_Agent_Included", "Other_Agent_Assumptions", "Empirical_Research", 
    "Country", "Flexible_Price_Assumption", "Exogenous_Inflation", "Households_discount_factor", 
    "Consumption_curvature_parameter", "Disutility_of_labor", 
    "Inverse_of_labor_supply_elasticity", "Money_curvature_parameter", 
    "Loan_to_value_ratio", "Labor_share_of_output", "Depositors_discount_factor", 
    "Price_adjustment_cost", "Elasticity_of_substitution_between_goods", 
    "AR1_coefficient_of_TFP", "Std_dev_to_TFP_shock", "Zero_Lower_Bound", 
    "Results_Table", "Results_Inflation", "Results_Inflation_Assumption", 
    "Preferred_Estimate", "Reason_for_Preferred", "Std_Dev_Inflation", 
    "Interest_Rate", "Impact_Factor"
]

# Importuj prompty ze star√©ho skriptu
from paste import DOCUMENT_1_PROMPT, DOCUMENT_2_PROMPT, DOCUMENT_3_PROMPT


class HybridPDFAnalyzer:
    """Hybridn√≠ analyz√°tor vyu≈æ√≠vaj√≠c√≠ Prompt Caching a Extended Thinking"""
    
    def __init__(self, api_key: str, export_folder: str):
        self.api_key = api_key
        self.export_folder = export_folder
        self.client = anthropic.Anthropic(api_key=api_key)
        self.current_study_id = 1
        
        # Cache pro session data
        self.session_cache = {}
        self.cache_stats = {
            'cache_writes': 0,
            'cache_hits': 0,
            'total_saved_tokens': 0
        }
        
    def extract_pdf_content(self, pdf_path: str) -> str:
        """Extrahuje text z PDF souboru"""
        logger.info(f"üìÑ Extrahuji text z PDF: {os.path.basename(pdf_path)}")
        
        try:
            with open(pdf_path, 'rb') as file:
                pdf_reader = PyPDF2.PdfReader(file)
                text = ""
                
                for page_num in range(len(pdf_reader.pages)):
                    page = pdf_reader.pages[page_num]
                    text += page.extract_text()
                
                logger.info(f"‚úÖ Extrahov√°no {len(text)} znak≈Ø z {len(pdf_reader.pages)} stran")
                return text
                
        except Exception as e:
            logger.error(f"‚ùå Chyba p≈ôi ƒçten√≠ PDF: {e}")
            return ""
    
    def create_cached_system_prompt(self, pdf_content: str, doc_name: str) -> List[Dict]:
        """Vytvo≈ô√≠ system prompt s cache_control pro PDF obsah"""
        return [
            {
                "type": "text",
                "text": f"You are analyzing the academic paper: {doc_name}\n\n"
                       "Follow the extraction instructions precisely and extract data "
                       "for meta-analysis from the PDF content below."
            },
            {
                "type": "text",
                "text": f"PDF CONTENT:\n\n{pdf_content}",
                "cache_control": {"type": "ephemeral"}
            }
        ]
    
    def analyze_with_caching(self, system_prompt: List[Dict], user_prompt: str, 
                           query_name: str, use_thinking: bool = False,
                           thinking_budget: int = 4000) -> Dict[str, Any]:
        """
        Analyzuje s vyu≈æit√≠m Prompt Caching a volitelnƒõ Extended Thinking
        
        Args:
            system_prompt: System prompt s PDF obsahem (s cache_control)
            user_prompt: U≈æivatelsk√Ω dotaz (Document 1, 2, nebo 3 prompt)
            query_name: N√°zev dotazu pro logging
            use_thinking: Zda pou≈æ√≠t Extended Thinking
            thinking_budget: Budget token≈Ø pro thinking
        """
        
        logger.info(f"ü§ñ Spou≈°t√≠m {query_name} anal√Ωzu...")
        
        if use_thinking:
            logger.info(f"üß† Extended Thinking aktivov√°no (budget: {thinking_budget} token≈Ø)")
        
        try:
            # Nastaven√≠ model≈Ø - snadno mƒõniteln√©
            STANDARD_MODEL = CLAUDE_MODEL  # Claude 4 Opus pro standardn√≠ dotazy
            THINKING_MODEL = CLAUDE_MODEL  # Claude 4 Opus i pro Extended Thinking
            
            # P≈ôiprav parametry
            params = {
                "model": THINKING_MODEL if use_thinking else STANDARD_MODEL,
                "max_tokens": thinking_budget + 2000 if use_thinking else 8000,  # Mus√≠ b√Ωt vƒõt≈°√≠ ne≈æ thinking_budget
                "temperature": 1.0 if use_thinking else 0.1,
                "system": system_prompt,
                "messages": [{"role": "user", "content": user_prompt}]
            }
            
            # P≈ôidej thinking pokud je po≈æadov√°no
            if use_thinking:
                params["thinking"] = {
                    "type": "enabled",
                    "budget_tokens": thinking_budget
                }
            
            # Zavolej API s streaming pro dlouh√© operace
            if use_thinking:
                # Pro Extended Thinking pou≈æij streaming
                response = self.client.messages.create(**params, stream=True)
                # Zpracuj streamovanou odpovƒõƒè
                full_response = self._process_stream_response(response)
            else:
                # Pro standardn√≠ dotazy pou≈æij norm√°ln√≠ vol√°n√≠
                response = self.client.messages.create(**params)
            
            # Loguj cache statistiky
            usage = response.usage
            if hasattr(usage, 'cache_creation_input_tokens') and usage.cache_creation_input_tokens:
                self.cache_stats['cache_writes'] += usage.cache_creation_input_tokens
                logger.info(f"üìù Cache write: {usage.cache_creation_input_tokens} token≈Ø")
            
            if hasattr(usage, 'cache_read_input_tokens') and usage.cache_read_input_tokens:
                self.cache_stats['cache_hits'] += usage.cache_read_input_tokens
                saved = usage.cache_read_input_tokens * 0.9  # 90% √∫spora
                self.cache_stats['total_saved_tokens'] += saved
                logger.info(f"üí∞ Cache hit: {usage.cache_read_input_tokens} token≈Ø (u≈°et≈ôeno ~{saved:.0f})")
            
            # Zpracuj odpovƒõƒè
            if use_thinking:
                return self._parse_thinking_response(full_response)
            else:
                return self._parse_standard_response(response)
                
        except Exception as e:
            logger.error(f"‚ùå Chyba p≈ôi {query_name} anal√Ωze: {e}")
            return {'error': str(e)}
    
    def _process_stream_response(self, stream) -> Any:
        """Zpracuje streamovanou odpovƒõƒè z Extended Thinking"""
        thinking_content = []
        final_answer = ""
        
        for chunk in stream:
            if chunk.type == "content_block_delta":
                if chunk.delta.type == "thinking":
                    thinking_content.append(chunk.delta.thinking)
                elif chunk.delta.type == "text_delta":
                    final_answer += chunk.delta.text
        
        # Vytvo≈ô√≠me mock response objekt pro kompatibilitu
        class MockResponse:
            def __init__(self, content):
                self.content = content
        
        return MockResponse([type('obj', (object,), {'type': 'text', 'text': final_answer})()])
    
    def _parse_thinking_response(self, response) -> Dict[str, Any]:
        """Parsuje odpovƒõƒè s thinking blocks"""
        thinking_content = []
        final_answer = ""
        
        for block in response.content:
            if block.type == "thinking":
                thinking_content.append(block.thinking)
            elif block.type == "text":
                final_answer += block.text
        
        return self._parse_response(final_answer)
    
    def _parse_standard_response(self, response) -> Dict[str, Any]:
        """Parsuje standardn√≠ odpovƒõƒè"""
        text = response.content[0].text if response.content else ""
        return self._parse_response(text)
    
    def _parse_response(self, text: str) -> Dict[str, Any]:
        """Parsuje textovou odpovƒõƒè na tabulkov√° data"""
        lines = text.strip().split('\n')
        
        # Hled√°me zaƒç√°tek tabulky
        table_start = -1
        for i, line in enumerate(lines):
            if 'Idstudy\t' in line or line.startswith('Idstudy'):
                table_start = i
                break
        
        if table_start >= 0:
            # Extrahujeme ≈ô√°dky tabulky
            table_rows = []
            for line in lines[table_start + 1:]:
                if '\t' in line:
                    cols = line.split('\t')
                    if len(cols) == len(META_ANALYSIS_COLUMNS):
                        table_rows.append(cols)
            
            return {'table_rows': table_rows}
        else:
            return {'raw_text': text}
    
    def analyze_pdf_hybrid(self, pdf_path: str) -> pd.DataFrame:
        """
        Hybridn√≠ anal√Ωza PDF s optimalizac√≠ pomoc√≠ Prompt Caching
        
        Strategie:
        1. Prvn√≠ dotaz (metadata) - ustanov√≠ cache
        2. Druh√Ω dotaz (model structure) - vyu≈æije cache hit
        3. T≈ôet√≠ dotaz (results) - vyu≈æije cache hit + Extended Thinking pro slo≈æitou extrakci
        """
        
        logger.info(f"\n{'='*60}")
        logger.info(f"üìö Analyzuji PDF: {os.path.basename(pdf_path)}")
        logger.info(f"{'='*60}")
        
        # 1. Extrahuj obsah PDF
        pdf_content = self.extract_pdf_content(pdf_path)
        if not pdf_content:
            logger.error(f"‚ùå Nepoda≈ôilo se extrahovat obsah z {os.path.basename(pdf_path)}")
            return self._create_empty_dataframe(self.current_study_id)
        
        # 2. Vytvo≈ô cached system prompt
        doc_name = os.path.basename(pdf_path)
        system_prompt = self.create_cached_system_prompt(pdf_content, doc_name)
        
        # 3. Document 1: Metadata (ustanov√≠ cache)
        logger.info("\nüìã Document 1: Extrakce metadat (ustanoven√≠ cache)")
        results1 = self.analyze_with_caching(
            system_prompt=system_prompt,
            user_prompt=DOCUMENT_1_PROMPT,
            query_name="Document 1 (Metadata)",
            use_thinking=False  # Jednoduch√©, nepot≈ôebuje thinking
        )
        
        # 4. Document 2: Model Structure (vyu≈æije cache)
        logger.info("\nüìã Document 2: Struktura modelu (cache hit)")
        time.sleep(2)  # Kr√°tk√° pauza
        results2 = self.analyze_with_caching(
            system_prompt=system_prompt,
            user_prompt=DOCUMENT_2_PROMPT,
            query_name="Document 2 (Model Structure)",
            use_thinking=False  # Strukturovan√©, ale ne p≈ô√≠li≈° slo≈æit√©
        )
        
        # 5. Document 3: Results & Parameters (cache hit + thinking)
        logger.info("\nüìã Document 3: V√Ωsledky a parametry (cache hit + extended thinking)")
        time.sleep(2)
        results3 = self.analyze_with_caching(
            system_prompt=system_prompt,
            user_prompt=DOCUMENT_3_PROMPT,
            query_name="Document 3 (Results)",
            use_thinking=True,  # Slo≈æit√© - extrakce v≈°ech inflaƒçn√≠ch v√Ωsledk≈Ø
            thinking_budget=8000  # Vy≈°≈°√≠ budget pro d≈Økladnou anal√Ωzu
        )
        
        # 6. Slouƒçit v√Ωsledky
        df = self.merge_results(results1, results2, results3, self.current_study_id)
        
        # 7. Zobrazit statistiky cache
        self._print_cache_statistics()
        
        return df
    
    def merge_results(self, results1: Dict, results2: Dict, results3: Dict, study_id: int) -> pd.DataFrame:
        """Slouƒç√≠ v√Ωsledky ze 3 dotaz≈Ø do jednoho DataFrame"""
        
        # Z√≠sk√°me poƒçet ≈ô√°dk≈Ø z t≈ôet√≠ho dotazu
        if 'table_rows' in results3 and results3['table_rows']:
            num_rows = len(results3['table_rows'])
        else:
            num_rows = 1
        
        logger.info(f"üìä Sluƒçuji v√Ωsledky: {num_rows} inflaƒçn√≠ch odhad≈Ø")
        
        # Vytvo≈ô√≠me pr√°zdn√Ω DataFrame
        df = pd.DataFrame(index=range(num_rows), columns=META_ANALYSIS_COLUMNS)
        df.fillna('NA', inplace=True)
        
        # Napln√≠me data z jednotliv√Ωch dotaz≈Ø
        # Document 1 - metadata (stejn√° pro v≈°echny ≈ô√°dky)
        if 'table_rows' in results1 and results1['table_rows']:
            doc1_data = results1['table_rows'][0]
            doc1_cols = ['Idstudy', 'Author', 'Author_Affiliation', 'DOI', 'Journal_Name', 
                        'Num_Citations', 'Year', 'Base_Model_Type', 'Country', 'Impact_Factor']
            
            for col_idx, col_name in enumerate(doc1_cols):
                if col_idx < len(doc1_data):
                    df[col_name] = doc1_data[col_idx]
        
        # Document 2 - model structure
        if 'table_rows' in results2 and results2['table_rows']:
            doc2_data = results2['table_rows'][0]
            doc2_mapping = {
                'Augmented_base_model': 9,
                'Augmentation_Description': 10,
                'Ramsey_Rule': 11,
                'HH_Included': 12,
                'Firms_Included': 13,
                'Banks_Included': 14,
                'Government_Included': 15,
                'HH_Maximization_Type': 16,
                'HH_Maximized_Vars': 17,
                'Producer_Type': 18,
                'Producer_Assumption': 19,
                'Other_Agent_Included': 20,
                'Other_Agent_Assumptions': 21,
                'Empirical_Research': 22,
                'Flexible_Price_Assumption': 24,
                'Exogenous_Inflation': 25,
                'Zero_Lower_Bound': 38
            }
            
            for col_name, col_idx in doc2_mapping.items():
                if col_idx < len(doc2_data):
                    df[col_name] = doc2_data[col_idx]
        
        # Document 3 - results & parameters (r≈Øzn√© pro ka≈æd√Ω ≈ô√°dek)
        if 'table_rows' in results3 and results3['table_rows']:
            for row_idx, row_data in enumerate(results3['table_rows']):
                if row_idx < num_rows:
                    doc3_mapping = {
                        'IdEstimate': 1,
                        'Households_discount_factor': 26,
                        'Consumption_curvature_parameter': 27,
                        'Disutility_of_labor': 28,
                        'Inverse_of_labor_supply_elasticity': 29,
                        'Money_curvature_parameter': 30,
                        'Loan_to_value_ratio': 31,
                        'Labor_share_of_output': 32,
                        'Depositors_discount_factor': 33,
                        'Price_adjustment_cost': 34,
                        'Elasticity_of_substitution_between_goods': 35,
                        'AR1_coefficient_of_TFP': 36,
                        'Std_dev_to_TFP_shock': 37,
                        'Zero_Lower_Bound': 38,
                        'Results_Table': 39,
                        'Results_Inflation': 40,
                        'Results_Inflation_Assumption': 41,
                        'Preferred_Estimate': 42,
                        'Reason_for_Preferred': 43,
                        'Std_Dev_Inflation': 44,
                        'Interest_Rate': 45
                    }
                    
                    for col_name, col_idx in doc3_mapping.items():
                        if col_idx < len(row_data):
                            df.loc[row_idx, col_name] = row_data[col_idx]
        
        # Nastav√≠me spr√°vn√© Idstudy
        df['Idstudy'] = str(study_id)
        
        # Vyƒçist√≠me data
        df = df.replace('', 'NA')
        df = df.fillna('NA')
        
        return df
    
    def _create_empty_dataframe(self, study_id: int) -> pd.DataFrame:
        """Vytvo≈ô√≠ pr√°zdn√Ω DataFrame s NA hodnotami"""
        empty_row = {col: 'NA' for col in META_ANALYSIS_COLUMNS}
        empty_row['Idstudy'] = str(study_id)
        empty_row['IdEstimate'] = '1'
        return pd.DataFrame([empty_row])
    
    def _print_cache_statistics(self):
        """Zobraz√≠ statistiky vyu≈æit√≠ cache"""
        logger.info("\nüí∞ Cache statistiky:")
        logger.info(f"  ‚Ä¢ Cache writes: {self.cache_stats['cache_writes']} token≈Ø")
        logger.info(f"  ‚Ä¢ Cache hits: {self.cache_stats['cache_hits']} token≈Ø")
        logger.info(f"  ‚Ä¢ U≈°et≈ôeno token≈Ø: ~{self.cache_stats['total_saved_tokens']:.0f}")
        
        if self.cache_stats['cache_hits'] > 0:
            efficiency = (self.cache_stats['total_saved_tokens'] / 
                         (self.cache_stats['cache_writes'] + self.cache_stats['cache_hits'])) * 100
            logger.info(f"  ‚Ä¢ Efektivita cache: {efficiency:.1f}%")
    
    def process_folder_hybrid(self, folder_path: str) -> pd.DataFrame:
        """Zpracuje v≈°echny PDF ve slo≈æce s hybridn√≠ optimalizac√≠"""
        
        pdf_files = list(Path(folder_path).glob("*.pdf"))
        
        if not pdf_files:
            logger.warning("Ve slo≈æce nebyly nalezeny ≈æ√°dn√© PDF soubory")
            return pd.DataFrame(columns=META_ANALYSIS_COLUMNS)
        
        logger.info(f"üìö Nalezeno {len(pdf_files)} PDF soubor≈Ø pro zpracov√°n√≠")
        
        all_results = []
        successful_count = 0
        failed_count = 0
        
        for idx, pdf_path in enumerate(pdf_files, 1):
            print(f"\n{'='*80}")
            print(f"üìÑ Zpracov√°v√°m {idx}/{len(pdf_files)}: {pdf_path.name}")
            print(f"{'='*80}")
            
            try:
                # Pauza mezi soubory
                if idx > 1:
                    wait_time = 5
                    print(f"‚è≥ ƒåek√°m {wait_time} sekund p≈ôed dal≈°√≠m souborem...")
                    time.sleep(wait_time)
                
                # Analyzuj PDF s hybridn√≠ metodou
                df_study = self.analyze_pdf_hybrid(str(pdf_path))
                
                # Kontrola v√Ωsledk≈Ø
                if df_study.empty or (len(df_study) == 1 and all(df_study.iloc[0] == 'NA')):
                    logger.warning(f"‚ö†Ô∏è ≈Ω√°dn√° data extrahov√°na z {pdf_path.name}")
                    failed_count += 1
                else:
                    all_results.append(df_study)
                    successful_count += 1
                    print(f"‚úÖ √öspƒõ≈°nƒõ zpracov√°no: {len(df_study)} inflaƒçn√≠ch odhad≈Ø")
                
                # Zv√Ω≈°√≠me ID pro dal≈°√≠ studii
                self.current_study_id += 1
                
            except Exception as e:
                logger.error(f"‚ùå Neoƒçek√°van√° chyba p≈ôi zpracov√°n√≠ {pdf_path.name}: {e}")
                failed_count += 1
                continue
        
        # Fin√°ln√≠ statistiky
        print(f"\n{'='*80}")
        print(f"üìä Zpracov√°n√≠ dokonƒçeno:")
        print(f"  ‚úÖ √öspƒõ≈°nƒõ: {successful_count}")
        print(f"  ‚ùå Ne√∫spƒõ≈°nƒõ: {failed_count}")
        print(f"  üìÅ Celkem: {len(pdf_files)}")
        print(f"{'='*80}")
        
        # Celkov√© cache statistiky
        self._print_cache_statistics()
        
        # Spoj√≠me v≈°echny v√Ωsledky
        if all_results:
            final_df = pd.concat(all_results, ignore_index=True)
            logger.info(f"\n‚úÖ Celkem {successful_count} √∫spƒõ≈°n√Ωch studi√≠ s {len(final_df)} ≈ô√°dky")
            return final_df
        else:
            logger.warning("Nebyly z√≠sk√°ny ≈æ√°dn√© v√Ωsledky")
            return pd.DataFrame(columns=META_ANALYSIS_COLUMNS)


def main():
    """Hlavn√≠ funkce"""
    print("=" * 80)
    print(" INFLATION META-ANALYSIS v6.0 - Hybrid Caching Architecture ".center(80, "="))
    print("=" * 80)
    print("\nüöÄ Hybridn√≠ architektura: Prompt Caching + Extended Thinking")
    print("üìä Dramatick√© sn√≠≈æen√≠ n√°klad≈Ø d√≠ky cache hits")
    print("üß† Inteligentn√≠ anal√Ωza s Extended Thinking pro slo≈æit√© extrakce")
    print("üí∞ 90% √∫spora token≈Ø p≈ôi opakovan√Ωch dotazech\n")
    
    # V√Ωbƒõr slo≈æky s PDF soubory
    print("üìÅ Vyberte slo≈æku obsahuj√≠c√≠ PDF soubory...")
    root = tk.Tk()
    root.withdraw()
    
    pdf_folder = filedialog.askdirectory(
        title="Vyberte slo≈æku s PDF soubory pro anal√Ωzu"
    )
    
    if not pdf_folder:
        print("‚ùå Nebyla vybr√°na slo≈æka")
        root.destroy()
        return
    
    print(f"‚úÖ Vybran√° slo≈æka: {pdf_folder}")
    
    # V√Ωbƒõr slo≈æky pro ulo≈æen√≠ v√Ωsledk≈Ø
    print("\nüìÅ Vyberte slo≈æku pro ulo≈æen√≠ v√Ωsledk≈Ø...")
    
    script_dir = os.path.dirname(os.path.abspath(__file__))
    default_export = os.path.join(script_dir, "AI_export_v6_hybrid")
    
    export_folder = filedialog.askdirectory(
        title="Vyberte slo≈æku pro ulo≈æen√≠ v√Ωsledk≈Ø",
        initialdir=script_dir
    )
    
    root.destroy()
    
    if not export_folder:
        export_folder = default_export
        print(f"üíæ Pou≈æiji v√Ωchoz√≠ slo≈æku: {export_folder}")
    else:
        print(f"üíæ Vybran√° slo≈æka: {export_folder}")
    
    # Vytvo≈ô√≠me slo≈æku pokud neexistuje
    os.makedirs(export_folder, exist_ok=True)
    
    # P≈ôid√°me file handler pro logging
    log_file = os.path.join(export_folder, f"processing_log_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.log")
    file_handler = logging.FileHandler(log_file, encoding='utf-8')
    file_handler.setFormatter(logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))
    logger.addHandler(file_handler)
    
    # Inicializace
    analyzer = HybridPDFAnalyzer(CLAUDE_API_KEY, export_folder)
    
    try:
        # Zpracov√°n√≠ v≈°ech PDF ve slo≈æce
        print("\nüöÄ Spou≈°t√≠m hybridn√≠ zpracov√°n√≠ s Prompt Caching...")
        final_df = analyzer.process_folder_hybrid(pdf_folder)
        
        if final_df.empty:
            print("\n‚ùå Nebyly z√≠sk√°ny ≈æ√°dn√© v√Ωsledky k ulo≈æen√≠")
            return
        
        # Zobrazen√≠ souhrnu v√Ωsledk≈Ø
        print("\n" + "=" * 80)
        print(" SOUHRN V√ùSLEDK≈Æ ".center(80, "="))
        print("=" * 80)
        
        # Statistiky
        unique_studies = final_df['Idstudy'].nunique()
        total_estimates = len(final_df)
        
        print(f"\nüìä Statistiky:")
        print(f"  ‚Ä¢ Poƒçet studi√≠: {unique_studies}")
        print(f"  ‚Ä¢ Celkem inflaƒçn√≠ch odhad≈Ø: {total_estimates}")
        print(f"  ‚Ä¢ Pr≈Ømƒõr odhad≈Ø na studii: {total_estimates/unique_studies:.1f}")
        
        # P≈ôehled studi√≠
        print(f"\nüìö P≈ôehled studi√≠:")
        for study_id in sorted(final_df['Idstudy'].unique()):
            study_data = final_df[final_df['Idstudy'] == study_id].iloc[0]
            study_count = len(final_df[final_df['Idstudy'] == study_id])
            author = study_data['Author'] if study_data['Author'] != 'NA' else 'Nezn√°m√Ω autor'
            year = study_data['Year'] if study_data['Year'] != 'NA' else '????'
            print(f"  {study_id}. {author} ({year}) - {study_count} odhad≈Ø")
        
        # Ulo≈æen√≠ v√Ωsledk≈Ø
        print("\nüíæ Ukl√°d√°m souhrnn√© v√Ωsledky...")
        
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # Excel export s detailn√≠mi listy
        excel_path = os.path.join(export_folder, f"meta_analysis_v6_hybrid_{timestamp}.xlsx")
        with pd.ExcelWriter(excel_path, engine='openpyxl') as writer:
            # Hlavn√≠ data
            final_df.to_excel(writer, sheet_name='Meta-Analysis', index=False)
            
            # Souhrn
            summary_df = pd.DataFrame({
                'Metric': ['Total Studies', 'Total Estimates', 'Average per Study', 
                          'Processing Date', 'Architecture'],
                'Value': [unique_studies, total_estimates, f"{total_estimates/unique_studies:.1f}", 
                         timestamp, 'Hybrid Caching v6.0']
            })
            summary_df.to_excel(writer, sheet_name='Summary', index=False)
            
            # Cache statistiky
            cache_df = pd.DataFrame({
                'Metric': ['Cache Writes (tokens)', 'Cache Hits (tokens)', 
                          'Tokens Saved', 'Cache Efficiency'],
                'Value': [
                    analyzer.cache_stats['cache_writes'],
                    analyzer.cache_stats['cache_hits'],
                    f"~{analyzer.cache_stats['total_saved_tokens']:.0f}",
                    f"{(analyzer.cache_stats['total_saved_tokens'] / (analyzer.cache_stats['cache_writes'] + analyzer.cache_stats['cache_hits']) * 100):.1f}%" if analyzer.cache_stats['cache_hits'] > 0 else "N/A"
                ]
            })
            cache_df.to_excel(writer, sheet_name='Cache_Statistics', index=False)
        
        print(f"‚úÖ Excel ulo≈æen: {excel_path}")
        
        # CSV backup
        csv_path = os.path.join(export_folder, f"meta_analysis_v6_hybrid_{timestamp}.csv")
        final_df.to_csv(csv_path, index=False, encoding='utf-8-sig')
        print(f"‚úÖ CSV backup: {csv_path}")
        
        print("\nüéâ Hybridn√≠ zpracov√°n√≠ dokonƒçeno!")
        print("‚ú® Pou≈æita kombinace Prompt Caching + Extended Thinking")
        print(f"üí∞ U≈°et≈ôeno ~{analyzer.cache_stats['total_saved_tokens']:.0f} token≈Ø d√≠ky cache hits")
        
    except Exception as e:
        logger.error(f"Kritick√° chyba: {e}")
        print(f"‚ùå Chyba: {e}")


if __name__ == "__main__":
    main()
