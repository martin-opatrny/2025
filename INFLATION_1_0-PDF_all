#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
INFLATION_META_ANALYSIS_v4_batch.py - Native PDF Support Batch Version
Zpracuje v≈°echny PDF soubory ve slo≈æce a vytvo≈ô√≠ jeden souhrnn√Ω Excel
"""

import logging
import sys
import os
import tkinter as tk
from tkinter import filedialog
import pandas as pd
import re
import requests
import time
import json
import base64
from bs4 import BeautifulSoup
import anthropic
import datetime
from typing import List, Dict, Optional, Tuple, Any
import numpy as np
from pathlib import Path

# Nastaven√≠ loggingu
logging.basicConfig(
    level=logging.DEBUG,  # Zmƒõnƒõno na DEBUG pro v√≠ce informac√≠
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout)
        # FileHandler p≈ôid√°me a≈æ v main() funkci
    ]
)
logger = logging.getLogger(__name__)
# Nastav√≠me vy≈°≈°√≠ √∫rove≈à pro nƒõkter√© knihovny, aby nezahlcovaly v√Ωstup
logging.getLogger('anthropic').setLevel(logging.WARNING)
logging.getLogger('httpx').setLevel(logging.WARNING)
logging.getLogger('urllib3').setLevel(logging.WARNING)

# Claude API kl√≠ƒç
CLAUDE_API_KEY = ""

# Model
CLAUDE_MODEL = "claude-opus-4-20250514"

# V√Ωstupn√≠ slo≈æka - bude nastavena v main() funkci
EXPORT_FOLDER = None

# Definice sloupc≈Ø
META_ANALYSIS_COLUMNS = [
    "Idstudy", "IdEstimate", "Author", "Author_Affiliation", "Journal_Name", 
    "Num_Citations", "Year", "Base_Model_Type", "Augmented_base_model", 
    "Augmentation_Description", "Ramsey_Rule", "HH_Included", "Firms_Included", 
    "Banks_Included", "Government_Included", "HH_Maximization_Type", 
    "HH_Maximized_Vars", "Producer_Type", "Producer_Assumption", 
    "Other_Agent_Included", "Other_Agent_Assumptions", "Empirical_Research", 
    "Country", "Flexible_Price_Assumption", "Exogenous_Inflation", "Households_discount_factor", 
    "Consumption_curvature_parameter", "Disutility_of_labor", 
    "Inverse_of_labor_supply_elasticity", "Money_curvature_parameter", 
    "Loan_to_value_ratio", "Labor_share_of_output", "Depositors_discount_factor", 
    "Price_adjustment_cost", "Elasticity_of_substitution_between_goods", 
    "AR1_coefficient_of_TFP", "Std_dev_to_TFP_shock", "Zero_Lower_Bound", 
    "Results_Table", "Results_Inflation", "Results_Inflation_Assumption", 
    "Preferred_Estimate", "Reason_for_Preferred", "Std_Dev_Inflation", 
    "Interest_Rate", "Impact_Factor"
]

class PDFAnalyzer:
    """Hlavn√≠ t≈ô√≠da pro anal√Ωzu PDF dokument≈Ø s nativn√≠ podporou"""
    
    def __init__(self, api_key: str, export_folder: str):
        self.api_key = api_key
        self.export_folder = export_folder
        self.client = anthropic.Anthropic(api_key=api_key)
        self.current_study_id = 1  # Pro spr√°vn√© ƒç√≠slov√°n√≠ studi√≠
        
    def analyze_pdf_native(self, pdf_path: str) -> Dict[str, Any]:
        """Analyzuje PDF pomoc√≠ nativn√≠ podpory Claude"""
        
        logger.info(f"ü§ñ Analyzuji: {os.path.basename(pdf_path)}")
        
        # Naƒçteme PDF jako base64
        with open(pdf_path, "rb") as f:
            pdf_data = base64.standard_b64encode(f.read()).decode("utf-8")
        
        # Vytvo≈ô√≠me kompletn√≠ prompt
        complete_prompt = self._create_complete_extraction_prompt()
        
        # Vytvo≈ô√≠me nov√Ω klient pro ka≈æd√© vol√°n√≠ (ƒçist√Ω kontext)
        fresh_client = anthropic.Anthropic(api_key=self.api_key)
        
        # Po≈°leme request s PDF dokumentem
        try:
            response = fresh_client.messages.create(
                model=CLAUDE_MODEL,
                max_tokens=8000,
                temperature=0.1,
                messages=[
                    {
                        "role": "user",
                        "content": [
                            {
                                "type": "document",
                                "source": {
                                    "type": "base64",
                                    "media_type": "application/pdf",
                                    "data": pdf_data
                                }
                                # Odstranil jsem cache_control pro d√°vkov√© zpracov√°n√≠
                            },
                            {
                                "type": "text",
                                "text": complete_prompt
                            }
                        ]
                    }
                ]
            )
            
            text = response.content[0].text
            logger.info(f"‚úÖ Odpovƒõƒè p≈ôijata pro {os.path.basename(pdf_path)}")
            
            # Zpracujeme odpovƒõƒè
            return self._parse_response(text)
            
        except Exception as e:
            logger.error(f"Chyba p≈ôi anal√Ωze {os.path.basename(pdf_path)}: {e}")
            return {'error': str(e)}
    
    def _parse_response(self, text: str) -> Dict[str, Any]:
        """Parsuje odpovƒõƒè od Claude"""
        
        # Debug log
        logger.debug(f"D√©lka odpovƒõdi: {len(text)} znak≈Ø")
        
        lines = text.strip().split('\n')
        
        # Hled√°me zaƒç√°tek tabulky (header)
        table_start = -1
        for i, line in enumerate(lines):
            if 'Idstudy\t' in line or line.startswith('Idstudy'):
                table_start = i
                logger.debug(f"Nalezen header tabulky na ≈ô√°dku {i}")
                break
        
        if table_start >= 0:
            # Extrahujeme ≈ô√°dky tabulky
            table_rows = []
            for line in lines[table_start + 1:]:
                if '\t' in line:
                    cols = line.split('\t')
                    if len(cols) == len(META_ANALYSIS_COLUMNS):
                        table_rows.append(cols)
                    else:
                        logger.warning(f"≈ò√°dek m√° {len(cols)} sloupc≈Ø m√≠sto {len(META_ANALYSIS_COLUMNS)}")
            
            logger.debug(f"Extrahov√°no {len(table_rows)} ≈ô√°dk≈Ø dat")
            return {'table_rows': table_rows}
        else:
            # Pokud nenajdeme tabulku, vr√°t√≠me raw text
            logger.warning("Tabulka nebyla nalezena v odpovƒõdi")
            # Zkus√≠me naj√≠t alespo≈à nƒõjak√© strukturovan√© informace
            if len(text) > 100:
                logger.debug(f"Prvn√≠ ƒç√°st odpovƒõdi: {text[:500]}...")
            return {'raw_text': text}
    
    def _create_complete_extraction_prompt(self) -> str:
        """Vytvo≈ô√≠ kompletn√≠ prompt pro extrakci v≈°ech dat"""
        return """
 AI-Optimized Meta-Analysis Research Assistant Instructions (Enhanced v3)

## CRITICAL SYSTEM DIRECTIVES

### MANDATORY COMPLIANCE REQUIREMENTS
- **NEVER leave any cell empty** - use "NA" for missing information
- **NEVER fabricate data** - only report verifiable information from sources
- **ALWAYS use exact column names** as specified below
- **ALWAYS follow the 0/1 coding system** for Yes/No questions (0 = No, 1 = Yes)
- **ALWAYS search required sources** (Google Scholar, Resurchify) when specified
- **ALWAYS extract ALL inflation results** found in the paper
- **ALWAYS verify variable identification** using notation sections and context
- **ALWAYS reset assumptions for each new table** - never carry over structure assumptions

### OUTPUT FORMAT REQUIREMENTS
- Generate a table with exact column headers provided
- Each row = one inflation result from the paper  
- Fill every cell with either data or "NA"
- Use tab-separated format for easy Excel import
- Include header row with all column names

## TASK OVERVIEW
Extract data from academic PDF articles for meta-analysis dataset creation. Process the attached PDF systematically using the numbered questions below. Each question corresponds to a specific Excel column.

## CRITICAL NOTATION GUIDE (READ FIRST)

### ENHANCED VARIABLE IDENTIFICATION PROTOCOL

**Inflation Variables - Extended Recognition:**
- **Standard notations**: œÄ, œÄ*, œÄe, œÄÃÑ, phi, œÜ, Œ†
- **Expected value notation**: E[œÄ], E(œÄ), ùîº[œÄ] - these ARE inflation values
- **Common labels**: "inflation", "inflation rate", "optimal inflation", "inflation target"
- **Typical range**: -5% to 10% (annualized)
- **Context clues**: Often discussed with price stability, welfare costs
- **WARNING - Verify these carefully**:
  - V[œÄ], Var[œÄ], œÉ¬≤(œÄ) - likely variance, NOT inflation value
  - SD[œÄ], œÉ(œÄ) - likely standard deviation
  - Cov[œÄ,x] - covariance, not inflation
  
**Verification Protocol for Ambiguous Notation:**
1. **Check surrounding text**: How is this value discussed?
2. **Check units**: Variance would be in (%)¬≤, inflation in %
3. **Check magnitude**: Variance typically much smaller than inflation
4. **Check table headers**: Often specify "mean" vs "variance"
5. **When uncertain**: Look for explicit definition in text

**Decision Rule**: 
- E[œÄ] ‚Üí Extract as inflation
- V[œÄ] ‚Üí Only extract if explicitly confirmed as inflation estimator, not variance

**Interest Rate Variables:**
- Common symbols: i, r, R, i*, r*, ƒ´, rÃÑ
- Common labels: "interest rate", "nominal rate", "policy rate", "real rate"
- Typical range: 0% to 15% (annualized)
- Context clues: Often discussed with monetary policy, Taylor rule

**Output Variables:**
- Common symbols: y, Y, ≈∑, y*, GDP
- Common labels: "output", "output gap", "GDP", "production"
- Context clues: Often discussed with business cycles, welfare

**CRITICAL**: Always check the paper's notation section or variable definitions first!

## DATA EXTRACTION FRAMEWORK

### 1. AUTHOR INFORMATION

**1.1 (INVARIANT)** 
- **Task**: Extract author names in APA format
- **Format**: Last name, First Initial. (Year). Example: "Smith, J. (2023)"
- **Source**: PDF article header/title page
- **Output**: Text string
- **Column**: `Author`

**1.2 (INVARIANT)**
- **Task**: Find author institutional affiliations
- **Process**: 1) Check PDF title page, footnotes, and author information sections 2) Search author names + "affiliation" + "university" on web if missing 3) Check author's recent publications for consistent affiliation
- **Format**: Institution name only, comma-separated for multiple authors
- **If not found**: Write "Cannot find affiliation"
- **Column**: `Author_Affiliation`

### 2. JOURNAL INFORMATION

**2.1 (INVARIANT)**
- **Task**: Extract journal name
- **Source**: PDF header, first page, or citation info
- **Format**: Exact journal name as appears in PDF
- **If not found**: "Cannot find journal name"
- **Column**: `Journal_Name`

**2.2 (INVARIANT)**
- **Task**: Find journal impact factor
- **Source**: Search Resurchify.com for exact journal name
- **Process**: 1) Go to Resurchify 2) Search exact journal name 3) Find "Impact score" number
- **Format**: Numerical value only
- **If not found**: "Cannot find impact factor"
- **Column**: `Impact_Factor`

### 3. CITATION INFORMATION

**3.1 (INVARIANT)**
- **Task**: Count citation numbers
- **Source**: Google Scholar search
- **Process**: 1) Search article title + author names 2) Find "Cited by" number
- **Format**: Number only
- **If not found**: "Cannot find citation count"
- **Column**: `Num_Citations`

### 4. PUBLICATION DATE

**4.1 (INVARIANT)**
- **Task**: Extract publication year
- **Source**: Search throughout entire PDF - title page, header, footer, reference section, journal information, copyright notice
- **Process**: 1) Check title page first 2) Check headers/footers on multiple pages 3) Check reference section 4) Check journal citation format anywhere in document
- **Format**: 4-digit year only
- **Column**: `Year`

### 5. RESEARCH SPECIFICATIONS

**5.1 (INVARIANT)**
- **Task**: Identify base economic model type
- **Examples**: "DSGE", "New Keynesian", "RBC", "VAR"
- **Source**: PDF abstract, introduction, or methodology section
- **Column**: `Base_Model_Type`

**5.2 (INVARIANT)**
- **Task**: Determine if model has augmentations
- **Process**: Look for model modifications, extensions, or additions
- **Output**: 1 (Yes) or 0 (No)
- **Column**: `Augmented_base_model`

**5.3 (VARIANT)**
- **Task**: Describe model augmentations
- **Condition**: Only if 5.2 = 1, otherwise "NA"
- **Format**: Brief description (2-5 words)
- **Column**: `Augmentation_Description`

**5.4 (VARIANT)**
- **Task**: Identify if the model uses a **Ramsey approach** to jointly determine optimal fiscal and monetary policy
- **Core Question**: Does the model find the optimal inflation rate by treating it as one of several **distortionary taxes** used to fund the government, because **lump-sum taxes are unavailable**?
- **Look for (Positive Signals)**:
  - Phrases: "Ramsey taxation", "optimal tax system", "inflation tax", "seigniorage", "public finance approach"
  - Assumptions: An explicit "government budget constraint" must be financed by "distortionary taxes"; the model states that "lump-sum taxes" are NOT available
  - Citations to authors like Phelps, Lucas & Stokey in the context of optimal policy
  - Discussion of inflation as a revenue source for government
  - Joint optimization of fiscal and monetary instruments
- **Rule out if (Negative Signals)**:
  - The model assumes "lump-sum taxes" are available (this usually points to the "Friedman Rule")
  - The central bank minimizes an "ad-hoc" or "quadratic loss function" (e.g., stabilizing inflation around a given target and an output gap) that is not derived from a government financing problem
  - Optimal inflation is determined solely by monetary frictions without fiscal considerations
  - The paper focuses only on monetary policy without fiscal policy integration
- **Output**: 1 (Yes - Ramsey approach) or 0 (No - not Ramsey approach)
- **Column**: `Ramsey_Rule`

**5.5 (VARIANT)**
- **Task**: Identify household agents in model
- **Search terms**: "household", "consumers", "families"
- **Output**: 1 (Yes) or 0 (No)
- **Column**: `HH_Included`

**5.6 (VARIANT)**
- **Task**: Identify firm/entrepreneur agents
- **Search terms**: "firms", "entrepreneurs", "producers", "companies"
- **Output**: 1 (Yes) or 0 (No)
- **Column**: `Firms_Included`

**5.7 (VARIANT)**
- **Task**: Identify banking sector in model
- **Search terms**: "banks", "financial intermediaries", "banking sector"
- **Critical**: Re-read carefully if uncertain
- **Output**: 1 (Yes) or 0 (No)
- **Column**: `Banks_Included`

**5.8 (VARIANT)**
- **Task**: Identify government sector in model
- **Search terms**: "government", "fiscal policy", "public sector"
- **Output**: 1 (Yes) or 0 (No)
- **Column**: `Government_Included`

**5.9 (INVARIANT)**
- **Task**: Identify what households maximize
- **Common answers**: "utility", "welfare", "consumption plus leisure"
- **Format**: Short phrase (2-4 words)
- **Column**: `HH_Maximization_Type`

**5.10 (INVARIANT)**
- **Task**: List variables in household optimization
- **Common answers**: "consumption, labor", "consumption, money, leisure"
- **Format**: Comma-separated list
- **Column**: `HH_Maximized_Vars`

**5.11 (VARIANT)**
- **Task**: Identify all firm types in model
- **Process**: 1) Look for explicit model equations or mathematical formulations involving firms 2) Check if firms are actual decision-making agents with optimization problems 3) Don't count casual mentions - need evidence of firms as active model components
- **Examples**: "intermediate goods firms, final goods firms"
- **Format**: Comma-separated list
- **Robustness Check**: Verify firms have optimization problems, production functions, or decision variables
- **Column**: `Producer_Type`

**5.12 (VARIANT)**
- **Task**: Identify market structure assumptions for each firm type
- **Search terms**: "monopolistic competition", "perfect competition", "monopoly"
- **Format**: "firm type: assumption" format
- **Example**: "intermediate firms: monopolistic competition, final firms: perfect competition"
- **Column**: `Producer_Assumption`

**5.13 (VARIANT)**
- **Task**: Check for other agents beyond households/firms/banks/government
- **Exclusions**: Do not count households, firms, banks, government
- **Examples**: "labor unions", "central bank", "foreign sector"
- **Output**: 1 (Yes) or 0 (No)
- **Column**: `Other_Agent_Included`

**5.14 (VARIANT)**
- **Task**: Describe other agents' assumptions
- **Condition**: Only if 5.13 = 1, otherwise "NA"
- **Format**: Brief description
- **Column**: `Other_Agent_Assumptions`

**5.15 (INVARIANT)**
- **Task**: Determine if research uses empirical data/estimation
- **Process**: Look if the result is model output using some calibration (not empirical) or based on some regression analysis (empirical)
- **Output**: 1 (Yes) or 0 (No)
- **Column**: `Empirical_Research`

**5.16 (INVARIANT)**
- **Task**: Identify country/region of data used
- **Process**: Look for calibration data, empirical data sources
- **Examples**: "US", "Euro Area", "UK"
- **If none specified**: "NA"
- **Column**: `Country`

**5.17 (VARIANT)**
- **Task**: Check price flexibility assumption for each inflation result
- **Process**: For each inflation result, determine if prices are flexible
- **Output**: 1 (flexible prices) or 0 (sticky prices)
- **Column**: `Flexible_Price_Assumption`

**5.18 (VARIANT)**
- **Task**: Check if inflation is exogenous
- **Process**: Determine if inflation is set externally (exogenous) vs. determined by model (endogenous)
- **Search terms**: "exogenous inflation", "inflation target", "inflation assumption", "given inflation rate"
- **Output**: 1 (exogenous) or 0 (endogenous)
- **Column**: `Exogenous_Inflation`

**5.19 (VARIANT)**
- **Task**: Check for growth parameters
- **Search terms**: "growth rate", "trend growth", "g"
- **Output**: 1 (Yes) or 0 (No)

**5.20 (VARIANT)**
- **Task**: Check zero lower bound environment
- **Process**: Determine if zero lower bound environment is set in the model
- **Output**: 1 (Yes) or 0 (No)
- **Column**: `Zero_Lower_Bound`

### CRITICAL SECTION: RESULTS EXTRACTION WITH TABLE RESET PROTOCOL

**5.21 (VARIANT)**
- **Task**: Identify and map all results locations
- **Enhanced Process**: 
  1. Create a mental map of ALL tables, figures, and text sections with numerical results
  2. Specifically note which tables contain inflation, interest rates, and other variables
  3. Check table titles, captions, and column headers carefully
  4. Note section numbers where results are discussed in text
- **Format**: "Table 1 (inflation & interest rates), Table 3 (welfare), Text Section 4.2"
- **Column**: `Results_Table`

### FIGURE EXTRACTION PROTOCOL

**Strict Rules for Figure-Based Results:**
1. **ONLY extract if exact numerical values are provided**:
   - ‚úì Values labeled on points
   - ‚úì Values in figure caption
   - ‚úì Values in figure legend
   - ‚úó Values estimated from visual inspection
   - ‚úó Values approximated from gridlines

2. **Acceptable figure sources**:
   - Bar charts with labeled values
   - Tables embedded in figures
   - Scatter plots with labeled points
   
3. **Unacceptable extractions**:
   - Line graphs without point labels
   - Heat maps without exact values
   - Any visual approximation

**Documentation**: In Results_Table column, specify: "Figure X (labeled values only)"

### MIXED CONTENT TABLE PROTOCOL

**Identify and Extract from Parameter Tables with Results:**
1. **Scan ALL tables** including those labeled "Parameters" or "Calibration"
2. **Check for embedded results**: Parameter tables may include:
   - "Implied inflation rate"
   - "Steady-state inflation"
   - "Optimal inflation"
   - Results in bottom rows or side columns
3. **Dual extraction**: From such tables, extract BOTH:
   - Parameters (for columns 5.27-5.38)
   - Inflation results (for Results_Inflation column)
4. **Flag source**: In Results_Table column, note: "Table X (parameters + results)"

## CRITICAL TABLE RESET PROTOCOL

### FOR EACH NEW TABLE ENCOUNTERED:

**MANDATORY RESET CHECKLIST:**
- [ ] **FORGET all assumptions from previous tables**
- [ ] **RE-READ all column headers from scratch**
- [ ] **RE-READ all row labels completely**
- [ ] **RE-VERIFY what each symbol means in THIS table**
- [ ] **CHECK table notes for any notation changes**
- [ ] **NEVER assume same column order as previous tables**

### MANDATORY EXTRACTION COMPLETENESS PROTOCOL

**For Tables with Multiple Results:**
1. **Count First, Extract Second**: Before extracting ANY values, count total rows/columns containing inflation results
2. **Create Extraction Checklist**: List each cell location (e.g., "Row 2 Col 3", "Row 3 Col 3") that contains an inflation value
3. **Systematic Extraction**: Work through checklist systematically, checking off each extraction
4. **Final Count Verification**: After extraction, verify extracted count matches initial count
5. **Missing Value Alert**: If counts don't match, re-examine table for missed values

**Large Table Strategy:**
- For tables with >10 results, process in batches of 5
- After each batch, verify against checklist
- Document progress: "Extracted rows 1-5 of 15 total"

### TABLE-SPECIFIC EXTRACTION PROCESS

**Step 1: Fresh Table Analysis (REQUIRED FOR EACH TABLE)**
1. **Treat each table as if it's the first table you've seen**
2. **Document the structure**: "Table X structure: Column 1 = [identify], Column 2 = [identify]"
3. **Note ANY differences from previous tables**
4. **Flag notation changes**: "Note: In this table, œÄ represents [what it represents here]"

**Step 2: Variable Identification for THIS Table**
1. **Check table caption and notes first**
2. **Identify what each column represents based on headers**
3. **Cross-reference with surrounding text that discusses THIS specific table**
4. **If symbols are ambiguous, find where this table is discussed in text**

**Step 3: Extraction with Anomaly Detection**

### ANOMALY DETECTION RULES (AUTOMATIC FLAGS):

**Flag Type 1: Value Range Anomalies**
- ‚ö†Ô∏è If inflation < -5% or > 10%, double-check extraction
- ‚ö†Ô∏è If interest rate < 0% or > 20%, double-check extraction
- ‚ö†Ô∏è If inflation suddenly jumps more than 5 percentage points from previous result
- ‚ö†Ô∏è If interest rate suddenly jumps more than 10 percentage points from previous result

**Flag Type 2: Relationship Anomalies**
- ‚ö†Ô∏è If nominal interest rate < inflation rate (violates typical Fisher equation)
- ‚ö†Ô∏è If real interest rate implied by results is < -10% or > 15%
- ‚ö†Ô∏è If inflation and interest rate have opposite signs (both should typically be positive)

**Flag Type 3: Cross-Table Inconsistencies**
- ‚ö†Ô∏è If baseline inflation in Table X differs from baseline inflation in Table Y by >1%
- ‚ö†Ô∏è If same model specification gives different results in different tables
- ‚ö†Ô∏è If table structure suddenly changes mid-paper

**Response to Anomaly Flags:**
When ANY anomaly flag triggers:
1. **STOP extraction for that value**
2. **Re-read the entire table context**
3. **Check if you've confused variables**
4. **Verify units (%, decimal, basis points)**
5. **Look for table notes explaining the anomaly**
6. **Only proceed if you can explain why the anomaly is correct**

**5.22 INFLATION EXTRACTION (With Reset Protocol)**
For EACH table containing results:
1. **Execute Table Reset Protocol first**
2. **Apply anomaly detection rules**
3. **Extract inflation values only after verification**
- **Column**: `Results_Inflation`

**5.40 INTEREST RATE EXTRACTION (With Reset Protocol)**
For EACH table containing results:
1. **Ensure you've reset assumptions for this table**
2. **Find interest rate that corresponds to each inflation value**
3. **Apply anomaly detection rules**
4. **Verify Fisher equation relationship makes sense**
- **Column**: `Interest_Rate`

### EXTRACTION VERIFICATION PROTOCOL

After extracting from EACH table:
1. **Internal Consistency Check**: Do all values from this table make economic sense together?
2. **Cross-Table Warning**: If values differ significantly from previous tables, document why
3. **Final Verification**: Re-read table one more time to ensure correct extraction

**5.23 (INVARIANT)**
- **Task**: Count total distinct inflation results
- **Enhanced Process**: 
  1. Count all unique inflation values across ALL tables and text
  2. Include baseline, sensitivity analysis, and robustness checks
  3. Each distinct value = one row in final output
- **Output**: Total count
- **Verification**: This number should match your final row count

**5.24 (VARIANT)**
- **Task**: Identify author's preferred estimate
- **Enhanced identifiers**: 
  - "baseline result"
  - "main specification"
  - "preferred estimate"
  - Results emphasized in abstract/conclusion
  - Results used for policy implications
- **Output**: 1 (preferred) or 0 (not preferred)
- **Column**: `Preferred_Estimate`

**5.25 (VARIANT)**
- **Task**: Explain why estimate is preferred
- **Condition**: Only for preferred estimates (5.24 = 1)
- **Format**: 1-2 professional sentences
- **Column**: `Reason_for_Preferred`

**5.26 (VARIANT)**
- **Task**: Describe assumptions for each inflation result
- **Enhanced Process**: 
  1. Identify the specific model variant/calibration used
  2. Note key assumptions that differ from other results
  3. Focus on economically meaningful differences
- **Format**: Brief assumption description
- **Example**: "Baseline calibration with sticky prices"
- **Column**: `Results_Inflation_Assumption`

### ZERO VALUE PROTOCOL FOR PARAMETERS

**Critical**: Zero is a valid parameter value!
- **NEVER skip zero values** - they represent important model choices
- **Format zero as**: "0" or "0.0" (not "NA")
- **Common zero parameters**:
  - Money curvature = 0 (cashless economy)
  - Price adjustment cost = 0 (flexible prices)
  - AR(1) coefficient = 0 (no persistence)
- **Verification**: If you see zero, double-check it's intentional, not missing data

**5.27 (VARIANT)**
- **Task**: Extract households' discount factor
- **Process**: Search parameter tables/text for discount factor value
- **Format**: Numerical value or "NA" if not found
- **Column**: `Households_discount_factor`

**5.28 (VARIANT)**
- **Task**: Extract consumption curvature parameter
- **Process**: Search parameter tables/text for consumption curvature value
- **Format**: Numerical value or "NA" if not found
- **Column**: `Consumption_curvature_parameter`

**5.29 (VARIANT)**
- **Task**: Extract disutility of labor parameter
- **Process**: Search parameter tables/text for labor disutility value
- **Format**: Numerical value or "NA" if not found
- **Column**: `Disutility_of_labor`

**5.30 (VARIANT)**
- **Task**: Extract inverse of labor supply elasticity
- **Process**: Search parameter tables/text for labor supply elasticity value
- **Format**: Numerical value or "NA" if not found
- **Column**: `Inverse_of_labor_supply_elasticity`

**5.31 (VARIANT)**
- **Task**: Extract money curvature parameter
- **Process**: Search parameter tables/text for money curvature value
- **Format**: Numerical value or "NA" if not found
- **Column**: `Money_curvature_parameter`

**5.32 (VARIANT)**
- **Task**: Extract loan-to-value ratio
- **Process**: Search parameter tables/text for LTV ratio value
- **Format**: Numerical value or "NA" if not found
- **Column**: `Loan_to_value_ratio`

**5.33 (VARIANT)**
- **Task**: Extract labor share of output
- **Process**: Search parameter tables/text for labor share value
- **Format**: Numerical value or "NA" if not found
- **Column**: `Labor_share_of_output`

**5.34 (VARIANT)**
- **Task**: Extract depositors' discount factor
- **Process**: Search parameter tables/text for depositors' discount factor value
- **Format**: Numerical value or "NA" if not found
- **Column**: `Depositors_discount_factor`

**5.35 (VARIANT)**
- **Task**: Extract price adjustment cost
- **Process**: Search parameter tables/text for price adjustment cost value
- **Format**: Numerical value or "NA" if not found
- **Column**: `Price_adjustment_cost`

**5.36 (VARIANT)**
- **Task**: Extract elasticity of substitution between goods
- **Process**: Search parameter tables/text for elasticity of substitution value
- **Format**: Numerical value or "NA" if not found
- **Column**: `Elasticity_of_substitution_between_goods`

**5.37 (VARIANT)**
- **Task**: Extract AR(1) coefficient of TFP
- **Process**: Search parameter tables/text for TFP AR(1) coefficient value
- **Format**: Numerical value or "NA" if not found
- **Column**: `AR1_coefficient_of_TFP`

**5.38 (VARIANT)**
- **Task**: Extract standard deviation to TFP shock
- **Process**: Search parameter tables/text for TFP shock std dev value
- **Format**: Numerical value or "NA" if not found
- **Column**: `Std_dev_to_TFP_shock`

**5.39 (VARIANT)**
- **Task**: Extract standard deviation of inflation result
- **Enhanced Process**:
  1. Look for uncertainty measures near inflation results
  2. Check for confidence intervals, standard errors, or posterior distributions
  3. May appear in parentheses or separate column
- **Format**: Numerical value or "NA" if not found
- **Column**: `Std_Dev_Inflation`

### 6. STUDY IDENTIFICATION

**6.1 (INVARIANT)** - Assign sequential study ID ‚Üí **Column**: `Idstudy`
**6.2 (VARIANT)** - Assign sequential estimate ID for each inflation result ‚Üí **Column**: `IdEstimate`

## EXTRACTION QUALITY CONTROL

**Multi-Pass Verification Protocol:**
1. **First Pass**: Initial extraction following all protocols
2. **Second Pass**: Re-read each source table/figure to verify:
   - No results were skipped
   - No zeros were missed
   - No mixed tables overlooked
   - No ambiguous notations misinterpreted
3. **Final Check**: Count total results again

**Red Flag Checklist:**
- [ ] Did I skip any tables labeled "Parameters" without checking for results?
- [ ] Did I assume any zeros were "NA"?
- [ ] Did I extract from any figures without exact numbers?
- [ ] Did I assume V[œÄ] or similar was inflation without verification?
- [ ] Does my final count match all inflation values in the paper?

## ENHANCED VALIDATION CHECKLIST

### Pre-Extraction Validation:
- [ ] Located and read paper's notation section
- [ ] Created a reference list of what each symbol means in this paper
- [ ] Identified all tables/sections containing results

### Per-Table Validation:
- [ ] **Executed Table Reset Protocol for EACH table**
- [ ] **Executed Extraction Completeness Protocol for large tables**
- [ ] Documented any structure/notation changes from previous tables
- [ ] Applied anomaly detection rules to all extractions
- [ ] Verified economic relationships make sense

### Post-Extraction Validation:
- [ ] All inflation values fall within reasonable range (with documented exceptions)
- [ ] Interest rates are consistent with inflation values
- [ ] No unexplained anomalies remain
- [ ] Each inflation result has its own row
- [ ] All cells filled with data or "NA"
- [ ] Zero values properly recorded, not skipped

## CRITICAL REMINDERS

1. **NEVER carry assumptions between tables** - each table is a fresh start
2. **ALWAYS trigger anomaly checks** when values seem unusual
3. **DOCUMENT notation changes** between tables
4. **VERIFY economic relationships** make sense
5. **When in doubt, re-read the table** rather than guess
6. **ZEROS are valid values** - never skip them
7. **CHECK parameter tables for results** - they often contain inflation values
8. **ONLY extract exact numbers from figures** - no visual estimation

## COMMON PITFALLS TO AVOID

1. **Assumption Carryover**: Assuming Table 3 has same structure as Table 1
2. **Ignoring Notation Changes**: Missing when authors switch symbol meanings
3. **Skipping Anomaly Checks**: Not investigating unusual values
4. **Column Position Assumptions**: Assuming inflation is always in column 2
5. **Section Blindness**: Not recognizing that different sections use different conventions
6. **Zero Blindness**: Treating zero as missing data instead of valid parameter
7. **Parameter Table Oversight**: Not checking parameter tables for embedded results
8. **Figure Estimation**: Trying to estimate values from graphs instead of finding exact numbers
9. **Notation Confusion**: Assuming V[œÄ] is inflation without verification
10. **Incomplete Extraction**: Missing results in large tables

## REQUIRED OUTPUT FORMAT

### EXACT COLUMN ORDER (MANDATORY)
```
Idstudy	IdEstimate	Author	Author_Affiliation	Journal_Name	Num_Citations	Year	Base_Model_Type	Augmented_base_model	Augmentation_Description	Ramsey_Rule	HH_Included	Firms_Included	Banks_Included	Government_Included	HH_Maximization_Type	HH_Maximized_Vars	Producer_Type	Producer_Assumption	Other_Agent_Included	Other_Agent_Assumptions	Empirical_Research	Country	Flexible_Price_Assumption	Exogenous_Inflation	Households_discount_factor	Consumption_curvature_parameter	Disutility_of_labor	Inverse_of_labor_supply_elasticity	Money_curvature_parameter	Loan_to_value_ratio	Labor_share_of_output	Depositors_discount_factor	Price_adjustment_cost	Elasticity_of_substitution_between_goods	AR1_coefficient_of_TFP	Std_dev_to_TFP_shock	Zero_Lower_Bound	Results_Table	Results_Inflation	Results_Inflation_Assumption	Preferred_Estimate	Reason_for_Preferred	Std_Dev_Inflation	Interest_Rate	Impact_Factor
```

### PROCESSING WORKFLOW (WITH ENHANCED PROTOCOLS)
1. **Read PDF completely** and identify notation/variable definitions
2. **Map all results locations** (tables, figures, text) including parameter tables
3. **Answer questions 1.1-5.20** systematically
4. **For EACH table: Execute Table Reset Protocol**
5. **For large tables: Execute Extraction Completeness Protocol**
6. **Extract results with anomaly detection active**
7. **Apply Zero Value Protocol for parameters**
8. **Validate using enhanced checklist and quality control**
9. **Search external sources** when required
10. **Create multiple rows** for multiple inflation results
11. **Fill every cell** with data or "NA" (including zeros)
12. **Format as tab-separated table**
"""
    
    def process_results_to_dataframe(self, results: Dict[str, Any], pdf_path: str, study_id: int) -> pd.DataFrame:
        """Zpracuje v√Ωsledky do DataFrame s unik√°tn√≠m Idstudy"""
        
        if 'table_rows' in results and results['table_rows']:
            # M√°me tabulku - vytvo≈ô√≠me DataFrame
            try:
                df = pd.DataFrame(results['table_rows'], columns=META_ANALYSIS_COLUMNS)
                
                # Nastav√≠me spr√°vn√© Idstudy pro v≈°echny ≈ô√°dky t√©to studie
                df['Idstudy'] = str(study_id)
                
                # Vyƒçist√≠me data
                df = df.replace('', 'NA')
                df = df.fillna('NA')
                
                logger.info(f"‚úÖ Vytvo≈ôeno {len(df)} ≈ô√°dk≈Ø pro studii {study_id}")
                return df
            except Exception as e:
                logger.error(f"Chyba p≈ôi vytv√°≈ôen√≠ DataFrame: {e}")
                logger.debug(f"Poƒçet ≈ô√°dk≈Ø: {len(results['table_rows'])}")
                if results['table_rows']:
                    logger.debug(f"Poƒçet sloupc≈Ø v prvn√≠m ≈ô√°dku: {len(results['table_rows'][0])}")
                    logger.debug(f"Oƒçek√°van√Ω poƒçet sloupc≈Ø: {len(META_ANALYSIS_COLUMNS)}")
                
                # Pokus o opravu - vytvo≈ô√≠me DataFrame s dostupn√Ωmi daty
                if results['table_rows']:
                    # Vezmeme jen tolik sloupc≈Ø, kolik m√°me
                    min_cols = min(len(results['table_rows'][0]), len(META_ANALYSIS_COLUMNS))
                    truncated_data = [row[:min_cols] for row in results['table_rows']]
                    truncated_cols = META_ANALYSIS_COLUMNS[:min_cols]
                    
                    try:
                        df = pd.DataFrame(truncated_data, columns=truncated_cols)
                        # Dopln√≠me chybƒõj√≠c√≠ sloupce
                        for col in META_ANALYSIS_COLUMNS:
                            if col not in df.columns:
                                df[col] = 'NA'
                        
                        df['Idstudy'] = str(study_id)
                        df = df.replace('', 'NA')
                        df = df.fillna('NA')
                        
                        logger.warning(f"‚ö†Ô∏è ƒå√°steƒçnƒõ √∫spƒõ≈°n√© zpracov√°n√≠ - vytvo≈ôeno {len(df)} ≈ô√°dk≈Ø s omezen√Ωmi daty")
                        return df[META_ANALYSIS_COLUMNS]  # Vr√°t√≠me ve spr√°vn√©m po≈ôad√≠
                        
                    except Exception as e2:
                        logger.error(f"Selhala i opravn√° extrakce: {e2}")
        
        elif 'raw_text' in results:
            logger.info(f"üìù Dostali jsme raw text m√≠sto tabulky pro {os.path.basename(pdf_path)}")
            logger.debug(f"D√©lka textu: {len(results['raw_text'])} znak≈Ø")
            
        # Vytvo≈ô√≠me pr√°zdn√Ω DataFrame s NA hodnotami
        logger.warning(f"‚ö†Ô∏è Nepoda≈ôilo se extrahovat strukturovan√° data pro {os.path.basename(pdf_path)}")
        
        empty_row = {col: 'NA' for col in META_ANALYSIS_COLUMNS}
        empty_row['Idstudy'] = str(study_id)
        empty_row['IdEstimate'] = '1'
        
        return pd.DataFrame([empty_row])
    
    def search_google_scholar(self, title: str, authors: str) -> str:
        """Vyhled√° poƒçet citac√≠ na Google Scholar"""
        try:
            search_query = f"{title} {authors}".strip()
            if not search_query:
                return "NA"
                
            url = f"https://scholar.google.com/scholar?q={search_query}"
            
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            
            response = requests.get(url, headers=headers, timeout=10)
            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # Hled√°me citace
                for div in soup.find_all('div', class_='gs_fl'):
                    cited_match = re.search(r'Cited by (\d+)', div.text)
                    if cited_match:
                        return cited_match.group(1)
            
            return "NA"
            
        except Exception as e:
            logger.warning(f"Chyba p≈ôi hled√°n√≠ citac√≠: {e}")
            return "NA"
    
    def search_impact_factor(self, journal_name: str) -> str:
        """Vyhled√° impact factor ƒçasopisu"""
        try:
            if not journal_name or journal_name == "NA":
                return "NA"
                
            # Zn√°m√© impact faktory
            known_factors = {
                "Economic Inquiry": "1.540",
                # M≈Ø≈æete p≈ôidat dal≈°√≠ zn√°m√© ƒçasopisy
            }
            
            for known_journal, factor in known_factors.items():
                if known_journal.lower() in journal_name.lower():
                    return factor
            
            # Obecn√© hled√°n√≠
            search_query = f"{journal_name} impact factor"
            url = f"https://www.resurchify.com/search?q={search_query}"
            
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            
            response = requests.get(url, headers=headers, timeout=10)
            if response.status_code == 200:
                # Hled√°me impact factor v textu
                impact_match = re.search(r'Impact\s*[Ff]actor[:\s]+(\d+\.?\d*)', response.text)
                if impact_match:
                    return impact_match.group(1)
            
            return "NA"
            
        except Exception as e:
            logger.warning(f"Chyba p≈ôi hled√°n√≠ impact faktoru: {e}")
            return "NA"
    
    def process_folder(self, folder_path: str) -> pd.DataFrame:
        """Zpracuje v≈°echny PDF ve slo≈æce a vr√°t√≠ souhrnn√Ω DataFrame"""
        
        # Najdeme v≈°echny PDF soubory
        pdf_files = list(Path(folder_path).glob("*.pdf"))
        
        if not pdf_files:
            logger.warning("Ve slo≈æce nebyly nalezeny ≈æ√°dn√© PDF soubory")
            return pd.DataFrame(columns=META_ANALYSIS_COLUMNS)
        
        logger.info(f"üìö Nalezeno {len(pdf_files)} PDF soubor≈Ø pro zpracov√°n√≠")
        
        all_results = []
        successful_count = 0
        failed_count = 0
        
        for idx, pdf_path in enumerate(pdf_files, 1):
            print(f"\n{'='*60}")
            print(f"üìÑ Zpracov√°v√°m {idx}/{len(pdf_files)}: {pdf_path.name}")
            print(f"{'='*60}")
            
            # Kontrola velikosti
            file_size = pdf_path.stat().st_size / (1024 * 1024)  # MB
            print(f"üìè Velikost: {file_size:.2f} MB")
            
            if file_size > 32:
                logger.warning(f"‚ö†Ô∏è P≈òESKAKUJI: {pdf_path.name} je vƒõt≈°√≠ ne≈æ 32 MB limit")
                failed_count += 1
                continue
            
            try:
                # Del≈°√≠ pauza mezi requesty pro stabilitu
                if idx > 1:
                    wait_time = 5  # 5 sekund mezi requesty
                    print(f"‚è≥ ƒåek√°m {wait_time} sekund p≈ôed dal≈°√≠m requestem...")
                    time.sleep(wait_time)
                
                # Analyzujeme PDF
                results = self.analyze_pdf_native(str(pdf_path))
                
                # Kontrola v√Ωsledk≈Ø
                if 'error' in results:
                    logger.error(f"‚ùå Chyba API: {results['error']}")
                    failed_count += 1
                    continue
                
                # Zpracujeme v√Ωsledky
                df_study = self.process_results_to_dataframe(results, str(pdf_path), self.current_study_id)
                
                # Kontrola, zda m√°me validn√≠ data
                if df_study.empty or (len(df_study) == 1 and all(df_study.iloc[0] == 'NA')):
                    logger.warning(f"‚ö†Ô∏è ≈Ω√°dn√° data extrahov√°na z {pdf_path.name}")
                    failed_count += 1
                    
                    # Ulo≈æ√≠me debug info
                    debug_path = os.path.join(self.export_folder, f"debug_{pdf_path.stem}_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.json")
                    with open(debug_path, 'w', encoding='utf-8') as f:
                        json.dump(results, f, ensure_ascii=False, indent=2)
                    logger.info(f"üìù Debug info ulo≈æeno: {debug_path}")
                else:
                    # Dopln√≠me citace a impact factor pro prvn√≠ ≈ô√°dek studie
                    first_row = df_study.iloc[0]
                    
                    # Citace
                    if first_row['Author'] != 'NA':
                        citations = self.search_google_scholar("", first_row['Author'])
                        df_study['Num_Citations'] = citations
                        logger.info(f"  üìä Citace: {citations}")
                    
                    # Impact factor
                    if first_row['Journal_Name'] != 'NA':
                        impact = self.search_impact_factor(first_row['Journal_Name'])
                        df_study['Impact_Factor'] = impact
                        logger.info(f"  üìä Impact factor: {impact}")
                    
                    # P≈ôid√°me k celkov√Ωm v√Ωsledk≈Øm
                    all_results.append(df_study)
                    successful_count += 1
                    
                    print(f"‚úÖ √öspƒõ≈°nƒõ zpracov√°no: {len(df_study)} inflaƒçn√≠ch odhad≈Ø")
                
                # Zv√Ω≈°√≠me ID pro dal≈°√≠ studii
                self.current_study_id += 1
                
            except Exception as e:
                logger.error(f"‚ùå Neoƒçek√°van√° chyba p≈ôi zpracov√°n√≠ {pdf_path.name}: {e}")
                failed_count += 1
                
                # Ulo≈æ√≠me error info
                error_info = {
                    'file': pdf_path.name,
                    'error': str(e),
                    'type': type(e).__name__
                }
                error_path = os.path.join(self.export_folder, f"error_{pdf_path.stem}_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.json")
                with open(error_path, 'w', encoding='utf-8') as f:
                    json.dump(error_info, f, ensure_ascii=False, indent=2)
                
                continue
        
        # Fin√°ln√≠ statistiky
        print(f"\n{'='*60}")
        print(f"üìä Zpracov√°n√≠ dokonƒçeno:")
        print(f"  ‚úÖ √öspƒõ≈°nƒõ: {successful_count}")
        print(f"  ‚ùå Ne√∫spƒõ≈°nƒõ: {failed_count}")
        print(f"  üìÅ Celkem: {len(pdf_files)}")
        print(f"{'='*60}")
        
        # Spoj√≠me v≈°echny v√Ωsledky
        if all_results:
            final_df = pd.concat(all_results, ignore_index=True)
            logger.info(f"\n‚úÖ Celkem {successful_count} √∫spƒõ≈°n√Ωch studi√≠ s {len(final_df)} ≈ô√°dky")
            return final_df
        else:
            logger.warning("Nebyly z√≠sk√°ny ≈æ√°dn√© v√Ωsledky")
            return pd.DataFrame(columns=META_ANALYSIS_COLUMNS)

def main():
    """Hlavn√≠ funkce"""
    print("=" * 80)
    print(" INFLATION META-ANALYSIS v4.0 BATCH - Native PDF ".center(80, "="))
    print("=" * 80)
    print("\nD√°vkov√© zpracov√°n√≠ PDF soubor≈Ø s nativn√≠ podporou Claude Opus 4")
    print("V≈°echny v√Ωsledky budou ulo≈æeny do jednoho Excel souboru\n")
    
    # V√Ωbƒõr slo≈æky s PDF soubory
    print("üìÅ Vyberte slo≈æku obsahuj√≠c√≠ PDF soubory...")
    root = tk.Tk()
    root.withdraw()
    
    pdf_folder = filedialog.askdirectory(
        title="Vyberte slo≈æku s PDF soubory pro anal√Ωzu"
    )
    
    if not pdf_folder:
        print("‚ùå Nebyla vybr√°na slo≈æka")
        root.destroy()
        return
    
    print(f"‚úÖ Vybran√° slo≈æka: {pdf_folder}")
    
    # V√Ωbƒõr slo≈æky pro ulo≈æen√≠ v√Ωsledk≈Ø
    print("\nüìÅ Vyberte slo≈æku pro ulo≈æen√≠ v√Ωsledk≈Ø...")
    
    script_dir = os.path.dirname(os.path.abspath(__file__))
    default_export = os.path.join(script_dir, "AI_export_batch")
    
    export_folder = filedialog.askdirectory(
        title="Vyberte slo≈æku pro ulo≈æen√≠ v√Ωsledk≈Ø",
        initialdir=script_dir
    )
    
    root.destroy()
    
    if not export_folder:
        export_folder = default_export
        print(f"üíæ Pou≈æiji v√Ωchoz√≠ slo≈æku: {export_folder}")
    else:
        print(f"üíæ Vybran√° slo≈æka: {export_folder}")
    
    # Vytvo≈ô√≠me slo≈æku pokud neexistuje
    os.makedirs(export_folder, exist_ok=True)
    
    # Inicializace
    analyzer = PDFAnalyzer(CLAUDE_API_KEY, export_folder)
    
    try:
        # Zpracov√°n√≠ v≈°ech PDF ve slo≈æce
        print("\nüöÄ Spou≈°t√≠m d√°vkov√© zpracov√°n√≠...")
        final_df = analyzer.process_folder(pdf_folder)
        
        if final_df.empty:
            print("\n‚ùå Nebyly z√≠sk√°ny ≈æ√°dn√© v√Ωsledky k ulo≈æen√≠")
            return
        
        # Zobrazen√≠ souhrnu v√Ωsledk≈Ø
        print("\n" + "=" * 80)
        print(" SOUHRN V√ùSLEDK≈Æ ".center(80, "="))
        print("=" * 80)
        
        # Statistiky
        unique_studies = final_df['Idstudy'].nunique()
        total_estimates = len(final_df)
        
        print(f"\nüìä Statistiky:")
        print(f"  ‚Ä¢ Poƒçet studi√≠: {unique_studies}")
        print(f"  ‚Ä¢ Celkem inflaƒçn√≠ch odhad≈Ø: {total_estimates}")
        print(f"  ‚Ä¢ Pr≈Ømƒõr odhad≈Ø na studii: {total_estimates/unique_studies:.1f}")
        
        # P≈ôehled studi√≠
        print(f"\nüìö P≈ôehled studi√≠:")
        for study_id in sorted(final_df['Idstudy'].unique()):
            study_data = final_df[final_df['Idstudy'] == study_id].iloc[0]
            study_count = len(final_df[final_df['Idstudy'] == study_id])
            author = study_data['Author'] if study_data['Author'] != 'NA' else 'Nezn√°m√Ω autor'
            year = study_data['Year'] if study_data['Year'] != 'NA' else '????'
            print(f"  {study_id}. {author} ({year}) - {study_count} odhad≈Ø")
        
        # Ulo≈æen√≠ v√Ωsledk≈Ø
        print("\nüíæ Ukl√°d√°m souhrnn√© v√Ωsledky...")
        
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # Excel export
        excel_path = os.path.join(export_folder, f"meta_analysis_batch_{timestamp}.xlsx")
        with pd.ExcelWriter(excel_path, engine='openpyxl') as writer:
            # Hlavn√≠ data
            final_df.to_excel(writer, sheet_name='Meta-Analysis', index=False)
            
            # Souhrn
            summary_df = pd.DataFrame({
                'Metric': ['Total Studies', 'Total Estimates', 'Average per Study', 'Processing Date'],
                'Value': [unique_studies, total_estimates, f"{total_estimates/unique_studies:.1f}", timestamp]
            })
            summary_df.to_excel(writer, sheet_name='Summary', index=False)
            
            # Validace
            validation_df = pd.DataFrame({
                'Study_ID': final_df['Idstudy'].unique(),
                'Author': [final_df[final_df['Idstudy']==sid]['Author'].iloc[0] for sid in final_df['Idstudy'].unique()],
                'Estimates': [len(final_df[final_df['Idstudy']==sid]) for sid in final_df['Idstudy'].unique()],
                'Has_Parameters': [
                    'Yes' if final_df[final_df['Idstudy']==sid]['Households_discount_factor'].iloc[0] != 'NA' else 'No' 
                    for sid in final_df['Idstudy'].unique()
                ]
            })
            validation_df.to_excel(writer, sheet_name='Validation', index=False)
        
        print(f"‚úÖ Excel ulo≈æen: {excel_path}")
        
        # CSV backup
        csv_path = os.path.join(export_folder, f"meta_analysis_batch_{timestamp}.csv")
        final_df.to_csv(csv_path, index=False, encoding='utf-8-sig')
        print(f"‚úÖ CSV backup: {csv_path}")
        
        print("\nüéâ D√°vkov√© zpracov√°n√≠ dokonƒçeno!")
        print("‚ú® Pou≈æita nativn√≠ PDF podpora Claude Opus 4")
        
    except Exception as e:
        logger.error(f"Kritick√° chyba: {e}")
        print(f"‚ùå Chyba: {e}")

if __name__ == "__main__":
    main()
